#README.txt

The following are links to the datasets we use for training:

#googleNews: a dataset of articles from Google News, 100B tokens and 692K types (after removing words ocurring less than 5 times).
https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit

#wiki_giga.txt: the English Gigaword Fifth Edition corpus (released June 17, 2011). 4.3B tokens
# Data is not free.  Description of data found here:
https://catalog.ldc.upenn.edu/LDC2011T07

#wiki_giga_skipgram.txt: Gigaword5 + Wikipedia2014, a Wikipedia dump.  6B tokens.
# dumps available from Wikipedia here: 
http://en.wikipedia.org/wiki/Wikipedia:Database_download

#twitterGlove: 2B, 27B tokens.
#data described on the main GloVe page:
http://nlp.stanford.edu/projects/glove/